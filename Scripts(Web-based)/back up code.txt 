seg2.py : 

from ultralytics import YOLO
import cv2
import numpy as np
import time
import csv
import os
from datetime import datetime

# Load YOLOv8n model
model = YOLO("yolov8n.pt")

# Constants
CAMERA_FOV_M2 = 10 * 10  # 10x10 square meters
DENSITY_THRESHOLD = 0.05  # people per m¬≤
CSV_FILE = "density_log.csv"

# CSV Initialization
if not os.path.exists(CSV_FILE):
    with open(CSV_FILE, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Timestamp", "People Count", "Density", "Device Status"])

# Webcam setup
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("Error: Cannot access webcam.")
    exit()

print("Press 'q' to quit.")

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Run YOLOv8 inference
    results = model(frame, verbose=False)
    boxes = []
    people_count = 0

    # Extract person detections
    for r in results:
        for box in r.boxes:
            cls = int(box.cls[0])
            conf = float(box.conf[0])
            if model.names[cls] == "person" and conf > 0.5:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                boxes.append((x1, y1, x2, y2))
                people_count += 1

    # Calculate density
    density = people_count / CAMERA_FOV_M2
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    device_status = "ON" if density > DENSITY_THRESHOLD else "OFF"

    # Save to CSV
    with open(CSV_FILE, mode='a', newline='') as file:
        writer = csv.writer(file)
        writer.writerow([timestamp, people_count, f"{density:.4f}", device_status])

    # Draw bounding boxes and info
    for (x1, y1, x2, y2) in boxes:
        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)

    label = f"People: {people_count} | Density: {density:.4f} ppl/m¬≤"
    cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

    if device_status == "ON":
        cv2.putText(frame, "AC + Kiosks ON", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        print("üî¥ High Density: Turning ON devices")
    else:
        cv2.putText(frame, "AC + Kiosks OFF", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        print("üü¢ Low Density: Turning OFF devices")

    # Display the frame
    cv2.imshow("Mall Camera Feed", frame)

    # Exit on 'q' key press
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Cleanup
cap.release()
cv2.destroyAllWindows()

--------------------- ****************** -------------------------



updated seg2.py (where it use yolo8n to capture number of peoples in real time 
and use raspberry pi to operate the device to be turn on or turn off based on the signal from the gpio pin 17 )

# people_density_controller.py

from ultralytics import YOLO
import cv2
import numpy as np
import time
import csv
import os
from datetime import datetime
from gpio_controller import turn_on_device, turn_off_device, cleanup_gpio

# Load YOLOv8n model
model = YOLO("yolov8n.pt")

# Constants
CAMERA_FOV_M2 = 10 * 10  # 10x10 square meters
DENSITY_THRESHOLD = 0.05  # people per m¬≤
CSV_FILE = "density_log.csv"

# CSV Initialization
if not os.path.exists(CSV_FILE):
    with open(CSV_FILE, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Timestamp", "People Count", "Density", "Device Status"])

# Webcam setup
cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("Error: Cannot access webcam.")
    exit()

print("Press 'q' to quit.")

try:
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # Run YOLOv8 inference
        results = model(frame, verbose=False)
        boxes = []
        people_count = 0

        # Extract person detections
        for r in results:
            for box in r.boxes:
                cls = int(box.cls[0])
                conf = float(box.conf[0])
                if model.names[cls] == "person" and conf > 0.5:
                    x1, y1, x2, y2 = map(int, box.xyxy[0])
                    boxes.append((x1, y1, x2, y2))
                    people_count += 1

        # Calculate density
        density = people_count / CAMERA_FOV_M2
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        device_status = "ON" if density > DENSITY_THRESHOLD else "OFF"

        # Save to CSV
        with open(CSV_FILE, mode='a', newline='') as file:
            writer = csv.writer(file)
            writer.writerow([timestamp, people_count, f"{density:.4f}", device_status])

        # GPIO control based on density
        if device_status == "ON":
            print("üî¥ High Density: Turning ON devices")
            turn_on_device()
        else:
            print("üü¢ Low Density: Turning OFF devices")
            turn_off_device()

        # Draw bounding boxes and info
        for (x1, y1, x2, y2) in boxes:
            cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)

        label = f"People: {people_count} | Density: {density:.4f} ppl/m¬≤"
        cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        if device_status == "ON":
            cv2.putText(frame, "AC + Kiosks ON", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
        else:
            cv2.putText(frame, "AC + Kiosks OFF", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

        # Display the frame
        cv2.imshow("Mall Camera Feed", frame)

        # Exit on 'q' key press
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

finally:
    cap.release()
    cv2.destroyAllWindows()
    cleanup_gpio()


--------------------- ****************** -------------------------


yolov8x-occlusion.pt
csonet_occlusion.pth





-----------------------------------------------***************8******8********8***********----------------------------------------

BACKUP CODE FOR TRAINGING THE RESNET MODEL WITH SHANGHAI DATASET 


import os
import numpy as np
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import torch.nn as nn
import torch.optim as optim
from torchvision.models import resnet50
import scipy.io
from scipy.ndimage import gaussian_filter

class ShanghaiTechDataset(Dataset):
    def __init__(self, root_dir, transform=None, train=True):
        """
        Args:
            root_dir (string): Directory with all the images and ground truth.
            transform (callable, optional): Optional transform to be applied on a sample.
            train (bool): If True, creates dataset from training set, else from test set.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.train = train
        
        # Get all image files from the images directory
        self.image_dir = os.path.join(self.root_dir, 'train_data' if train else 'test_data', 'images')
        self.image_files = sorted([f for f in os.listdir(self.image_dir) if f.endswith('.jpg')])
        
        # Ground truth directory
        self.gt_dir = os.path.join(self.root_dir, 'train_data' if train else 'test_data', 'ground-truth')
        
        print(f"Found {len(self.image_files)} images in {self.image_dir}")
        print(f"GT directory: {self.gt_dir}")
        
    def __len__(self):
        return len(self.image_files)
    
    def create_density_map(self, points, image_shape, sigma=15):
        """Create density map from point annotations"""
        h, w = image_shape[:2]
        density_map = np.zeros((h, w), dtype=np.float32)
        
        if len(points) == 0:
            return density_map
            
        # Add gaussian at each point location
        for point in points:
            x, y = int(point[0]), int(point[1])
            if 0 <= x < w and 0 <= y < h:
                # Create a small gaussian around the point
                y_start = max(0, y - 3*sigma)
                y_end = min(h, y + 3*sigma + 1)
                x_start = max(0, x - 3*sigma)
                x_end = min(w, x + 3*sigma + 1)
                
                for yi in range(y_start, y_end):
                    for xi in range(x_start, x_end):
                        distance_sq = (xi - x)**2 + (yi - y)**2
                        density_map[yi, xi] += np.exp(-distance_sq / (2 * sigma**2))
        
        return density_map
    
    def __getitem__(self, idx):
        img_name = self.image_files[idx]
        img_path = os.path.join(self.image_dir, img_name)
        
        # Load image
        image = Image.open(img_path).convert('RGB')
        original_size = image.size  # (width, height)
        
        # Load ground truth .mat file
        gt_name = 'GT_' + os.path.splitext(img_name)[0] + '.mat'
        gt_path = os.path.join(self.gt_dir, gt_name)
        
        try:
            gt_data = scipy.io.loadmat(gt_path)
            # The annotation points are usually stored in 'image_info' -> 'location' or similar
            if 'image_info' in gt_data:
                points = gt_data['image_info'][0][0][0][0][0]  # Navigate the nested structure
            elif 'annPoints' in gt_data:
                points = gt_data['annPoints']
            else:
                # Try to find any array-like data
                for key in gt_data.keys():
                    if not key.startswith('__') and isinstance(gt_data[key], np.ndarray):
                        points = gt_data[key]
                        break
                else:
                    points = np.array([])
            
            if len(points.shape) > 1 and points.shape[1] >= 2:
                points = points[:, :2]  # Take only x, y coordinates
            else:
                points = np.array([])
                
        except Exception as e:
            print(f"Error loading {gt_path}: {e}")
            points = np.array([])
        
        # Create density map
        density_map = self.create_density_map(points, (original_size[1], original_size[0]))
        count = len(points) if len(points.shape) > 1 else 0
        
        # Apply transforms to image
        if self.transform:
            image = self.transform(image)
            
        # Resize density map to match the transformed image size (224x224)
        if self.transform:
            density_map = Image.fromarray(density_map)
            density_map = density_map.resize((224, 224), Image.BICUBIC)
            density_map = np.array(density_map)
            # Scale the density values proportionally
            scale_factor = (224 * 224) / (original_size[0] * original_size[1])
            density_map = density_map * scale_factor
            
        # Convert density map to tensor
        density_map = torch.from_numpy(density_map).float().unsqueeze(0)  # Add channel dimension
        count = torch.tensor(count, dtype=torch.float32)
        
        return image, density_map, count

def create_data_loaders(batch_size=8, data_dir='/Users/yesvinv/SUMMER_INTERN_PROJ/try2/archive/ShanghaiTech'):
    """Create training and validation data loaders for ShanghaiTech dataset"""
    # Data augmentation for training
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),  # Fixed size for ResNet
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Validation transform
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    # Create datasets for both parts (A and B)
    train_dataset_a = ShanghaiTechDataset(
        os.path.join(data_dir, 'part_A'),
        transform=train_transform,
        train=True
    )
    train_dataset_b = ShanghaiTechDataset(
        os.path.join(data_dir, 'part_B'),
        transform=train_transform,
        train=True
    )
    
    val_dataset_a = ShanghaiTechDataset(
        os.path.join(data_dir, 'part_A'),
        transform=val_transform,
        train=False
    )
    val_dataset_b = ShanghaiTechDataset(
        os.path.join(data_dir, 'part_B'),
        transform=val_transform,
        train=False
    )
    
    # Combine datasets
    train_dataset = torch.utils.data.ConcatDataset([train_dataset_a, train_dataset_b])
    val_dataset = torch.utils.data.ConcatDataset([val_dataset_a, val_dataset_b])
    
    print(f"Training samples: {len(train_dataset)}")
    print(f"Validation samples: {len(val_dataset)}")
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # Set to 0 to avoid multiprocessing issues
        pin_memory=True if torch.backends.mps.is_available() else False
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,  # Set to 0 to avoid multiprocessing issues
        pin_memory=True if torch.backends.mps.is_available() else False
    )
    
    return train_loader, val_loader

def train_epoch(model, train_loader, criterion, optimizer, device):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0
    running_mae = 0.0
    
    for batch_idx, (data, density_maps, counts) in enumerate(train_loader):
        data, density_maps, counts = data.to(device), density_maps.to(device), counts.to(device)
        
        optimizer.zero_grad()
        output = model(data)  # This should output density maps
        
        # Calculate loss
        loss = criterion(output, density_maps)
        
        # Calculate MAE for counts
        pred_counts = torch.sum(output, dim=(2,3))  # Sum over H,W dimensions
        pred_counts = torch.sum(pred_counts, dim=1)  # Sum over channel dimension
        mae = torch.mean(torch.abs(pred_counts - counts))
        
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
        running_mae += mae.item()
        
        if batch_idx % 10 == 0:
            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}, MAE: {mae.item():.2f}')
    
    epoch_loss = running_loss / len(train_loader)
    epoch_mae = running_mae / len(train_loader)
    return epoch_loss, epoch_mae

def validate(model, val_loader, criterion, device):
    """Validate the model"""
    model.eval()
    val_loss = 0.0
    val_mae = 0.0
    
    with torch.no_grad():
        for data, density_maps, counts in val_loader:
            data, density_maps, counts = data.to(device), density_maps.to(device), counts.to(device)
            output = model(data)
            
            val_loss += criterion(output, density_maps).item()
            
            pred_counts = torch.sum(output, dim=(2,3))  # Sum over H,W dimensions
            pred_counts = torch.sum(pred_counts, dim=1)  # Sum over channel dimension
            val_mae += torch.mean(torch.abs(pred_counts - counts)).item()
    
    val_loss /= len(val_loader)
    val_mae /= len(val_loader)
    return val_loss, val_mae

class DensityResNet(nn.Module):
    def __init__(self):
        super().__init__()
        # Load pre-trained ResNet50
        self.base_model = resnet50(weights='IMAGENET1K_V2')
        
        # Remove the original fully connected layer and average pooling
        self.base_model.fc = nn.Identity()
        self.base_model.avgpool = nn.Identity()
        
        # Add custom head for density map prediction
        self.density_head = nn.Sequential(
            nn.Conv2d(2048, 1024, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 1, kernel_size=1),  # Output single channel density map
            nn.ReLU()  # Ensure non-negative outputs
        )
        
    def forward(self, x):
        # Get features from ResNet backbone
        x = self.base_model.conv1(x)
        x = self.base_model.bn1(x)
        x = self.base_model.relu(x)
        x = self.base_model.maxpool(x)
        
        x = self.base_model.layer1(x)
        x = self.base_model.layer2(x)
        x = self.base_model.layer3(x)
        x = self.base_model.layer4(x)
        
        # Pass through density head
        x = self.density_head(x)
        
        # Upsample to match input size (224x224)
        x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)
        
        return x

def main():
    """Main training function"""
    # Configuration
    config = {
        'batch_size': 4,  # Reduced for memory efficiency
        'learning_rate': 1e-5,  # Lower learning rate for fine-tuning
        'weight_decay': 1e-4,
        'epochs': 100,
        'data_dir': '/Users/yesvinv/SUMMER_INTERN_PROJ/try2/archive/ShanghaiTech',
        'save_path': 'resnet50_density_model.pth'
    }
    
    # Device setup - prioritize MPS for Mac GPU acceleration
    if torch.backends.mps.is_available():
        device = torch.device('mps')
        print('Using device: MPS (Mac GPU acceleration)')
    elif torch.cuda.is_available():
        device = torch.device('cuda')
        print('Using device: CUDA')
    else:
        device = torch.device('cpu')
        print('Using device: CPU')
    
    try:
        # Data loaders
        train_loader, val_loader = create_data_loaders(
            batch_size=config['batch_size'], 
            data_dir=config['data_dir']
        )
        
        # Model
        model = DensityResNet().to(device)
        
        # Loss function - MSE for density map regression
        criterion = nn.MSELoss()
        
        # Optimizer - only train the density head first
        optimizer = optim.Adam(
            model.density_head.parameters(),  # Only train the new head initially
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )
        
        # Training loop
        best_val_mae = float('inf')
        for epoch in range(config['epochs']):
            print(f'\nEpoch {epoch+1}/{config["epochs"]}')
            print('-' * 40)
            
            # Train
            train_loss, train_mae = train_epoch(model, train_loader, criterion, optimizer, device)
            
            # Validate
            val_loss, val_mae = validate(model, val_loader, criterion, device)
            
            print(f'Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.2f}')
            print(f'Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.2f}')
            
            # After 5 epochs, start fine-tuning the entire model
            if epoch == 5:
                optimizer = optim.Adam(
                    model.parameters(),  # Now train all parameters
                    lr=config['learning_rate']/10,  # Lower learning rate
                    weight_decay=config['weight_decay']
                )
                print("\nStarted fine-tuning entire model with lower learning rate")
            
            # Save best model
            if val_mae < best_val_mae:
                best_val_mae = val_mae
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_mae': val_mae,
                }, config['save_path'])
                print(f'New best model saved with val_mae: {val_mae:.2f}')
        
        print(f'\nTraining completed! Best validation MAE: {best_val_mae:.2f}')
        
    except Exception as e:
        print(f"Error during training: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()




===============================================================================================================================================

backup code for the seg12.py which is execute very well by using yolo but not the rsnet if we give resnet_path = false then it would be usibng the yolo effectively 


code : 

import os
import cv2
import time
import csv
import torch
import numpy as np
from PIL import Image
from datetime import datetime
from torchvision import transforms, models
from ultralytics import YOLO
from torch import nn
from gpio_controller import turn_on_device, turn_off_device, cleanup_gpio
from deep_sort_realtime.deepsort_tracker import DeepSort
import logging

# === Configuration ===
CAMERA_ID = 0  # Change to 1, 2, etc., if using multiple cameras
CAMERA_FOV_M2 = 10 * 10  # Area covered by camera (in m¬≤)
DENSITY_THRESHOLD = 0.05  # Trigger threshold (people per m¬≤)
CSV_FILE = "density_log.csv"
DATASET_DIR = "dataset"
OUTPUT_DIR = "output_results"
YOLO_MODEL_PATH = "yolov8n.pt"
RESNET_MODEL_PATH = "resnet50_density_model.pth"  # ‚ö†Ô∏è ADD YOUR RESNET MODEL HERE
MIN_CONFIDENCE = 0.3  # Minimum confidence for YOLO detections
FRAME_SKIP = 2  # Process every nth frame for performance
FORCE_GPU = True  # Set to True to force GPU usage
BATCH_SIZE = 4  # For batch processing ResNet inference

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === Create folders if they don't exist ===
os.makedirs(DATASET_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs('model', exist_ok=True)

# Ensure YOLO model path exists
RESNET_MODEL_PATH = "resnet50_density_model.pth"
os.makedirs(os.path.dirname(RESNET_MODEL_PATH), exist_ok=True)

# Initialize CSV file with headers if it doesn't exist
if not os.path.exists(CSV_FILE):
    with open(CSV_FILE, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Timestamp", "Camera ID", "People Count", "Density", "Device Status", 
                         "Occluded Detected", "Tracked IDs", "Processing Time (ms)"])

# === Helper Functions ===
def calculate_iou(box1, box2):
    """Calculate Intersection over Union between two bounding boxes"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    inter_area = max(0, x2 - x1) * max(0, y2 - y1)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    return inter_area / float(box1_area + box2_area - inter_area)

def check_mps_availability():
    if torch.backends.mps.is_available():
        print("üöÄ MPS is available. Using Apple Silicon GPU!")
        return torch.device("mps")
    else:
        print("‚ö†Ô∏è MPS not available. Falling back to CPU.")
        return torch.device("cpu")

device = check_mps_availability()

# === Load Models ===
gpu_available = check_mps_availability()
device = torch.device('mps' if gpu_available and FORCE_GPU else 'cpu')
logger.info(f"üéØ Using device: {device}")

# Load YOLOv8 model with GPU optimization
yolo_model = YOLO(YOLO_MODEL_PATH)
if device.type == 'mps':
    yolo_model.to(device)
    logger.info("‚úÖ YOLO model moved to GPU")

# === UPDATED RESNET MODEL SECTION ===
class ResNetOcclusionDetector(nn.Module):
    def __init__(self, num_classes=2):
        super(ResNetOcclusionDetector, self).__init__()
        # This matches the saved model structure
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),  # features.0
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # features.3
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((7, 7))
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(128 * 7 * 7, num_classes)  # classifier
        )
    
    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

class NewResNetOcclusionDetector(nn.Module):
    def __init__(self, num_classes=2):
        super(NewResNetOcclusionDetector, self).__init__()
        self.backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        num_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )
        
        # Freeze early layers for faster inference
        for param in list(self.backbone.parameters())[:-10]:
            param.requires_grad = False
    
    def forward(self, x):
        return self.backbone(x)

def inspect_saved_model(model_path):
    """Inspect the structure of your saved model"""
    try:
        state_dict = torch.load(model_path, map_location='cpu')
        print("=== SAVED MODEL STRUCTURE ===")
        for key, tensor in state_dict.items():
            print(f"{key}: {tensor.shape}")
        
        print("\n=== ARCHITECTURE ANALYSIS ===")
        if any('features' in key for key in state_dict.keys()):
            print("‚úÖ Model uses 'features' layers (simple CNN)")
        if any('backbone' in key for key in state_dict.keys()):
            print("‚úÖ Model uses 'backbone' layers (ResNet)")
        if any('classifier' in key for key in state_dict.keys()):
            print("‚úÖ Model has 'classifier' layer")
            
    except Exception as e:
        print(f"Error loading model: {e}")

def load_model_flexibly(model_path, device):
    """Try to load model with different architectures"""
    
    # First, inspect the saved model
    state_dict = torch.load(model_path, map_location=device)
    
    # Determine architecture based on keys
    if any('features' in key for key in state_dict.keys()):
        print("üîç Detected simple CNN architecture")
        model = ResNetOcclusionDetector()  # Use the corrected version above
    elif any('backbone' in key for key in state_dict.keys()):
        print("üîç Detected ResNet backbone architecture")
        model = NewResNetOcclusionDetector()
    else:
        print("‚ùå Unknown architecture")
        return None
    
    try:
        model.load_state_dict(state_dict)
        model.to(device)
        print("‚úÖ Model loaded successfully")
        return model
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return None

# =================== RESNET MODEL LOADING ===================
USE_RESNET = True  # Set to False if you don't want to use ResNet

def create_and_train_resnet_model():
    """
    üö® PLACEHOLDER FUNCTION - REPLACE WITH YOUR TRAINING CODE
    This function should train your ResNet model for occlusion detection
    You need to prepare a dataset with:
    - Class 0: Non-occluded persons
    - Class 1: Occluded persons
    """
    logger.warning("üö® Training function called but not implemented!")
    logger.info("üìù To train ResNet model:")
    logger.info("   1. Collect dataset of person crops (occluded vs non-occluded)")
    logger.info("   2. Organize in folders: dataset/train/0/ and dataset/train/1/")
    logger.info("   3. Use PyTorch training loop with this model architecture")
    logger.info("   4. Save trained model to: " + RESNET_MODEL_PATH)
    return None

# Load ResNet model
resnet_model = None
if USE_RESNET:
    try:
        # First inspect your model
        logger.info("üîç Inspecting saved model...")
        inspect_saved_model(RESNET_MODEL_PATH)
        
        # Try flexible loading
        resnet_model = load_model_flexibly(RESNET_MODEL_PATH, device)
        
        if resnet_model is None:
            raise Exception("Could not load model with any architecture")
            
        logger.info("‚úÖ ResNet model loaded successfully")
        if device.type == 'mps':
            logger.info("‚úÖ ResNet model moved to GPU")
            
    except FileNotFoundError:
        logger.error("‚ùå ResNet model not found at: " + RESNET_MODEL_PATH)
        logger.info("üîß Options:")
        logger.info("   1. Train a new model by calling create_and_train_resnet_model()")
        logger.info("   2. Set USE_RESNET = False to disable ResNet")
        logger.info("   3. Provide your trained model at the specified path")
        
        choice = input("Continue without ResNet? (y/n): ").lower()
        if choice == 'y':
            USE_RESNET = False
            resnet_model = None
            logger.info("üîÑ Continuing without ResNet model")
        else:
            logger.info("üõë Please add your ResNet model and restart")
            exit(1)
    except Exception as e:
        logger.error(f"‚ùå Error loading ResNet model: {e}")
        logger.info("üîß The model architecture doesn't match. Options:")
        logger.info("   1. Use the corrected architecture (Solution 1)")
        logger.info("   2. Retrain with new architecture (Solution 2)")
        logger.info("   3. Set USE_RESNET = False to disable ResNet")
        
        choice = input("Continue without ResNet? (y/n): ").lower()
        if choice == 'y':
            USE_RESNET = False
            resnet_model = None
            logger.info("üîÑ Continuing without ResNet model")
        else:
            logger.info("üõë Please fix the model architecture and restart")
            exit(1)
    
    if resnet_model:
        resnet_model.eval()
        # Optimize for inference
        if device.type == 'mps':
            resnet_model.half()  # Use half precision for faster inference
            logger.info("‚ö° ResNet model optimized with half precision")

# ResNet Preprocessing (optimized)
resnet_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Initialize DeepSORT tracker with GPU optimization
tracker = DeepSort(
    max_age=50,
    n_init=3,
    max_cosine_distance=0.4,
    nn_budget=None,
    embedder="mobilenet",
    half=True,
    bgr=True,
    embedder_gpu=device.type == 'mps',  # Use GPU for embeddings if available
    embedder_wts=None,
    polygon=False,
    today=None
)

# === Optimized Batch Processing for ResNet ===
def process_resnet_batch(crops, model, transform, device):
    """Process multiple crops in batch for better GPU utilization"""
    if not crops or model is None:
        return []
    
    # Prepare batch
    batch_tensors = []
    for crop in crops:
        if crop.size > 0:
            tensor = transform(crop).unsqueeze(0)
            if device.type == 'mps':
                tensor = tensor.half()  # Use half precision
            batch_tensors.append(tensor)
    
    if not batch_tensors:
        return []
    
    # Stack tensors
    batch = torch.cat(batch_tensors).to(device)
    
    # Inference
    with torch.no_grad():
        outputs = model(batch)
        probabilities = torch.softmax(outputs, dim=1)
        confidences = probabilities.max(dim=1)[0]
        predictions = torch.argmax(outputs, dim=1)
    
    return [(pred.item(), conf.item()) for pred, conf in zip(predictions, confidences)]

# === Main Processing Loop ===
def main():
    cap = cv2.VideoCapture(CAMERA_ID)
    if not cap.isOpened():
        logger.error(f"‚ùå Cannot access camera {CAMERA_ID}")
        return

    # Optimize camera settings
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Reduce buffer for real-time processing
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    logger.info("üì∑ Press 'q' to quit.")
    logger.info(f"üîç Using {'ResNet + DeepSORT' if USE_RESNET else 'YOLO + DeepSORT'} on {device}")

    frame_count = 0
    csv_file = open(CSV_FILE, mode='a', newline='')
    csv_writer = csv.writer(csv_file)
    
    # Performance tracking
    processing_times = []
    
    try:
        while True:
            start_time = time.time()
            ret, frame = cap.read()
            if not ret:
                logger.warning("‚ö†Ô∏è Camera error, attempting to reconnect...")
                cap.release()
                time.sleep(2)
                cap = cv2.VideoCapture(CAMERA_ID)
                continue

            frame_count += 1
            if frame_count % FRAME_SKIP != 0:
                continue

            timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
            raw_filename = f"cam{CAMERA_ID}_{timestamp}.jpg"
            
            # Save raw frame (optional - comment out to save disk space)
            # cv2.imwrite(os.path.join(DATASET_DIR, raw_filename), frame)

            # === Person Detection ===
            people_boxes = []
            occluded_count = 0
            
            # Run YOLO detection
            results = yolo_model(frame, verbose=False, conf=MIN_CONFIDENCE)
            
            # Collect crops for batch processing
            crops_for_resnet = []
            crop_indices = []
            
            for r in results:
                for box in r.boxes:
                    cls = int(box.cls[0])
                    conf = float(box.conf[0])
                    if yolo_model.names[cls] == "person":
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        
                        # Skip invalid boxes
                        if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 >= frame.shape[1] or y2 >= frame.shape[0]:
                            continue
                            
                        crop = frame[y1:y2, x1:x2]
                        if crop.size == 0:
                            continue

                        label = "YOLO"
                        final_conf = conf

                        # Check for potential occlusion
                        bbox_area = (x2 - x1) * (y2 - y1)
                        is_potentially_occluded = conf < 0.6 or bbox_area < 1500

                        if USE_RESNET and is_potentially_occluded:
                            crops_for_resnet.append(crop)
                            crop_indices.append(len(people_boxes))
                            people_boxes.append((x1, y1, x2, y2, label, final_conf, True))  # True = needs ResNet
                        else:
                            people_boxes.append((x1, y1, x2, y2, label, final_conf, False))  # False = no ResNet needed

            # Batch process ResNet
            if USE_RESNET and crops_for_resnet:
                resnet_results = process_resnet_batch(crops_for_resnet, resnet_model, resnet_transform, device)
                
                # Update people_boxes with ResNet results
                for i, (prediction, confidence) in enumerate(resnet_results):
                    if prediction == 1 and confidence > 0.5:  # Occluded person detected
                        box_idx = crop_indices[i]
                        x1, y1, x2, y2, _, _, _ = people_boxes[box_idx]
                        people_boxes[box_idx] = (x1, y1, x2, y2, "ResNet Refined", confidence, False)
                        occluded_count += 1

            # Clean up people_boxes (remove the ResNet flag)
            people_boxes = [(x1, y1, x2, y2, label, conf) for x1, y1, x2, y2, label, conf, flag in people_boxes if not flag or USE_RESNET]

            # === Tracking with DeepSORT ===
            formatted_detections = []
            for x1, y1, x2, y2, label, conf in people_boxes:
                formatted_detections.append(([x1, y1, x2, y2], conf, 0))  # 0 for person class

            tracks = tracker.update_tracks(formatted_detections, frame=frame)
            
            # Process tracks
            tracked_people = []
            active_track_ids = []
            for track in tracks:
                if not track.is_confirmed():
                    continue
                
                track_id = track.track_id
                ltrb = track.to_ltrb()
                x1, y1, x2, y2 = map(int, ltrb)
                active_track_ids.append(track_id)
                
                # Find best matching detection
                best_match = None
                best_iou = 0
                for det in people_boxes:
                    det_box = det[:4]
                    iou = calculate_iou(ltrb, det_box)
                    if iou > best_iou:
                        best_iou = iou
                        best_match = det
                
                if best_match and best_iou > 0.3:
                    label = f"{best_match[4]} (ID:{track_id})"
                    conf = best_match[5]
                else:
                    label = f"Tracked (ID:{track_id})"
                    conf = 0.7  # Default confidence for tracked objects
                
                tracked_people.append((x1, y1, x2, y2, label, conf))

            # Use tracked people count for density calculation
            people_count = len(tracked_people)
            density = people_count / CAMERA_FOV_M2
            device_status = "ON" if density > DENSITY_THRESHOLD else "OFF"

            # Control devices based on density
            try:
                if device_status == "ON":
                    turn_on_device()
                else:
                    turn_off_device()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è GPIO control error: {e}")

            # Log data
            processing_time = (time.time() - start_time) * 1000
            processing_times.append(processing_time)
            
            tracked_ids_str = ",".join(map(str, active_track_ids)) if active_track_ids else "None"
            csv_writer.writerow([
                timestamp.replace('_', ' '), 
                f"Camera {CAMERA_ID}",
                people_count, 
                f"{density:.4f}", 
                device_status, 
                occluded_count,
                tracked_ids_str,
                f"{processing_time:.2f}"
            ])

            # === Visualization ===
            for (x1, y1, x2, y2, label, conf) in tracked_people:
                # Color coding
                if "ResNet" in label:
                    color = (0, 0, 255)  # Red
                elif "Tracked" in label:
                    color = (0, 255, 0)  # Green
                else:
                    color = (255, 0, 0)  # Blue

                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(frame, f"{label} ({conf:.2f})", (x1, y1 - 5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

            # Enhanced status display
            avg_fps = 1000 / np.mean(processing_times[-30:]) if processing_times else 0
            cv2.putText(frame, f"People: {people_count} (Occluded: {occluded_count})",
                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            cv2.putText(frame, f"Density: {density:.4f} ppl/m¬≤ | Devices: {device_status}",
                        (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
            cv2.putText(frame, f"Tracks: {len(active_track_ids)} | FPS: {avg_fps:.1f} | Device: {device}",
                        (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
            cv2.putText(frame, f"ResNet: {'ON' if USE_RESNET else 'OFF'} | Frame: {frame_count}",
                        (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

            # Save and display
            cv2.imwrite(os.path.join(OUTPUT_DIR, raw_filename), frame)
            cv2.imshow("People Density Monitoring", frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

            # Periodic cleanup and logging
            if frame_count % 100 == 0:
                if device.type == 'mps':
                    torch.mps.empty_cache()
                avg_proc_time = np.mean(processing_times[-100:])
                logger.info(f"üìä Frame {frame_count}: Avg processing time: {avg_proc_time:.2f}ms, FPS: {1000/avg_proc_time:.1f}")

    except KeyboardInterrupt:
        logger.info("üõë Interrupted by user")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        csv_file.close()
        cleanup_gpio()
        logger.info("üõë Cleanup completed")
        
        # Final statistics
        if processing_times:
            avg_time = np.mean(processing_times)
            logger.info(f"üìà Average processing time: {avg_time:.2f}ms")
            logger.info(f"üìà Average FPS: {1000/avg_time:.1f}")

if __name__ == "__main__":
    main()

    ================================================================================================================


seg12.py :

updated code : 


import os
import cv2
import time
import csv
import torch
import numpy as np
from PIL import Image
from datetime import datetime
from torchvision import transforms, models
from ultralytics import YOLO
from torch import nn
from gpio_controller import turn_on_device, turn_off_device, cleanup_gpio
from deep_sort_realtime.deepsort_tracker import DeepSort
import logging


# === Configuration ===
CAMERA_ID = 0  # Change to 1, 2, etc., if using multiple cameras
CAMERA_FOV_M2 = 10 * 10  # Area covered by camera (in m¬≤)
DENSITY_THRESHOLD = 0.05  # Trigger threshold (people per m¬≤)
CSV_FILE = "density_log.csv"
DATASET_DIR = "dataset"
OUTPUT_DIR = "output_results"
YOLO_MODEL_PATH = "yolov8n.pt"
RESNET_MODEL_PATH = "model/resnet50_density_model.pth"  # ‚úÖ FIXED: Added directory path
MIN_CONFIDENCE = 0.3  # Minimum confidence for YOLO detections
FRAME_SKIP = 2  # Process every nth frame for performance
FORCE_GPU = True  # Set to True to force GPU usage
BATCH_SIZE = 4  # For batch processing ResNet inference

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === Create folders if they don't exist ===
os.makedirs(DATASET_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs('model', exist_ok=True)

# ‚úÖ FIXED: Only create directory if path contains a directory
if os.path.dirname(RESNET_MODEL_PATH):
    os.makedirs(os.path.dirname(RESNET_MODEL_PATH), exist_ok=True)

# Initialize CSV file with headers if it doesn't exist
if not os.path.exists(CSV_FILE):
    with open(CSV_FILE, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Timestamp", "Camera ID", "People Count", "Density", "Device Status", 
                         "Crowd Density", "Tracked IDs", "Processing Time (ms)"])

# === Helper Functions ===
def calculate_iou(box1, box2):
    """Calculate Intersection over Union between two bounding boxes"""
    x1 = max(box1[0], box2[0])
    y1 = max(box1[1], box2[1])
    x2 = min(box1[2], box2[2])
    y2 = min(box1[3], box2[3])
    
    inter_area = max(0, x2 - x1) * max(0, y2 - y1)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    
    return inter_area / float(box1_area + box2_area - inter_area)

def check_mps_availability():
    if torch.backends.mps.is_available():
        print("üöÄ MPS is available. Using Apple Silicon GPU!")
        return torch.device("mps")
    else:
        print("‚ö†Ô∏è MPS not available. Falling back to CPU.")
        return torch.device("cpu")

device = check_mps_availability()

# === Load Models ===
gpu_available = check_mps_availability()
device = torch.device('mps' if gpu_available and FORCE_GPU else 'cpu')
logger.info(f"üéØ Using device: {device}")

# Load YOLOv8 model with GPU optimization
yolo_model = YOLO(YOLO_MODEL_PATH)
if device.type == 'mps':
    yolo_model.to(device)
    logger.info("‚úÖ YOLO model moved to GPU")

# === CORRECTED RESNET MODEL SECTION ===
class DensityResNet(nn.Module):
    """
    ‚úÖ CORRECTED: This matches the exact architecture from custom_dataset.py
    """
    def __init__(self):
        super().__init__()
        # Load pre-trained ResNet50 - EXACTLY as in custom_dataset.py
        self.base_model = models.resnet50(weights='IMAGENET1K_V2')
        
        # Remove the original fully connected layer and average pooling
        self.base_model.fc = nn.Identity()
        self.base_model.avgpool = nn.Identity()
        
        # Add custom head for density map prediction - EXACTLY as in custom_dataset.py
        self.density_head = nn.Sequential(
            nn.Conv2d(2048, 1024, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 512, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(512, 256, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 1, kernel_size=1),  # Output single channel density map
            nn.ReLU()  # Ensure non-negative outputs
        )
        
    def forward(self, x):
        # Get features from ResNet backbone - EXACTLY as in custom_dataset.py
        x = self.base_model.conv1(x)
        x = self.base_model.bn1(x)
        x = self.base_model.relu(x)
        x = self.base_model.maxpool(x)
        
        x = self.base_model.layer1(x)
        x = self.base_model.layer2(x)
        x = self.base_model.layer3(x)
        x = self.base_model.layer4(x)
        
        # Pass through density head
        x = self.density_head(x)
        
        # Upsample to match input size (224x224)
        x = nn.functional.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)
        
        return x

class CrowdDensityClassifier(nn.Module):
    """
    ‚úÖ NEW: Convert density maps to crowd density classification
    This takes the DensityResNet output and converts it to crowd density categories
    """
    def __init__(self, density_model):
        super().__init__()
        self.density_model = density_model
        
        # Freeze the density model
        for param in self.density_model.parameters():
            param.requires_grad = False
        
        # Add classifier for crowd density levels
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling
            nn.Flatten(),
            nn.Linear(1, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 4),  # 4 classes: Low, Medium, High, Very High
            nn.Softmax(dim=1)
        )
        
    def forward(self, x):
        # Get density map from the trained model
        density_map = self.density_model(x)
        
        # Convert to crowd density classification
        crowd_level = self.classifier(density_map)
        
        return density_map, crowd_level

def load_trained_model(model_path, device):
    """
    ‚úÖ CORRECTED: Load the exact model architecture that was trained
    """
    try:
        # Create the exact model architecture
        model = DensityResNet()
        
        # Load the saved checkpoint
        checkpoint = torch.load(model_path, map_location=device)
        
        # Load state dict - handle both direct state dict and checkpoint format
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
            logger.info("‚úÖ Loaded checkpoint format")
        else:
            state_dict = checkpoint
            logger.info("‚úÖ Loaded direct state dict format")
        
        # Load the state dict
        model.load_state_dict(state_dict)
        model.to(device)
        model.eval()
        
        logger.info("‚úÖ DensityResNet model loaded successfully")
        logger.info(f"   ‚Ä¢ Model has {sum(p.numel() for p in model.parameters()):,} parameters")
        
        # Create the classifier wrapper
        classifier_model = CrowdDensityClassifier(model)
        classifier_model.to(device)
        classifier_model.eval()
        
        return model, classifier_model
        
    except Exception as e:
        logger.error(f"‚ùå Error loading model: {e}")
        return None, None

# =================== RESNET MODEL LOADING ===================
USE_RESNET = True  # Set to False if you don't want to use ResNet

def create_and_train_resnet_model():
    """
    üö® PLACEHOLDER FUNCTION - Your model is already trained!
    """
    logger.warning("üö® This function is not needed - your model is already trained!")
    logger.info("üìù Your trained model is at: " + RESNET_MODEL_PATH)
    logger.info("üìù It was trained using custom_dataset.py with ShanghaiTech dataset")
    return None

# Load ResNet model
resnet_model = None
crowd_classifier = None

if USE_RESNET:
    try:
        logger.info("üîç Loading trained DensityResNet model...")
        
        # Load the trained model
        resnet_model, crowd_classifier = load_trained_model(RESNET_MODEL_PATH, device)
        
        if resnet_model is None:
            raise Exception("Could not load the trained model")
            
        logger.info("‚úÖ ResNet density model loaded successfully")
        if device.type == 'mps':
            logger.info("‚úÖ ResNet model moved to GPU")
            
    except FileNotFoundError:
        logger.error("‚ùå ResNet model not found at: " + RESNET_MODEL_PATH)
        logger.info("üîß Options:")
        logger.info("   1. Make sure the model file exists at the specified path")
        logger.info("   2. Train the model using custom_dataset.py")
        logger.info("   3. Set USE_RESNET = False to disable ResNet")
        
        choice = input("Continue without ResNet? (y/n): ").lower()
        if choice == 'y':
            USE_RESNET = False
            resnet_model = None
            crowd_classifier = None
            logger.info("üîÑ Continuing without ResNet model")
        else:
            logger.info("üõë Please add your ResNet model and restart")
            exit(1)
    except Exception as e:
        logger.error(f"‚ùå Error loading ResNet model: {e}")
        logger.info("üîß Check if the model file is corrupted or has the wrong format")
        
        choice = input("Continue without ResNet? (y/n): ").lower()
        if choice == 'y':
            USE_RESNET = False
            resnet_model = None
            crowd_classifier = None
            logger.info("üîÑ Continuing without ResNet model")
        else:
            logger.info("üõë Please fix the model and restart")
            exit(1)

# ResNet Preprocessing (optimized for crowd density)
resnet_transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Initialize DeepSORT tracker with GPU optimization
tracker = DeepSort(
    max_age=50,
    n_init=3,
    max_cosine_distance=0.4,
    nn_budget=None,
    embedder="mobilenet",
    half=True,
    bgr=True,
    embedder_gpu=device.type == 'mps',  # Use GPU for embeddings if available
    embedder_wts=None,
    polygon=False,
    today=None
)

# === Optimized Batch Processing for ResNet ===
def analyze_crowd_density(frame, model, classifier, transform, device):
    """
    ‚úÖ NEW: Analyze overall crowd density of the entire frame
    """
    if model is None or classifier is None:
        return 0.0, "Unknown", 0.0
    
    try:
        # Preprocess the entire frame
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        input_tensor = transform(frame_rgb).unsqueeze(0).to(device)
        
        with torch.no_grad():
            # Get density map and crowd classification
            density_map, crowd_level = classifier(input_tensor)
            
            # Calculate total crowd count from density map
            total_count = torch.sum(density_map).item()
            
            # Get crowd density level
            crowd_probs = crowd_level[0]  # Remove batch dimension
            crowd_class = torch.argmax(crowd_probs).item()
            crowd_confidence = torch.max(crowd_probs).item()
            
            # Map class to label
            crowd_labels = ["Low", "Medium", "High", "Very High"]
            crowd_label = crowd_labels[crowd_class]
            
            return total_count, crowd_label, crowd_confidence
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error in crowd analysis: {e}")
        return 0.0, "Error", 0.0

def process_resnet_batch(crops, model, transform, device):
    """
    ‚úÖ UPDATED: Process person crops for individual density analysis
    Note: This is kept for compatibility, but the main crowd analysis 
    now uses the full frame analysis above
    """
    if not crops or model is None:
        return []
    
    # This function is now mainly for individual person analysis
    # The main crowd density analysis is done on the full frame
    results = []
    
    for crop in crops:
        if crop.size > 0:
            try:
                # For individual person crops, we can estimate if they're in a crowded area
                tensor = transform(crop).unsqueeze(0).to(device)
                
                with torch.no_grad():
                    density_output = model(tensor)
                    person_density = torch.sum(density_output).item()
                    
                    # Simple threshold for "crowded" person
                    is_crowded = person_density > 0.5
                    confidence = min(person_density, 1.0)
                    
                    results.append((1 if is_crowded else 0, confidence))
            except:
                results.append((0, 0.0))
        else:
            results.append((0, 0.0))
    
    return results

# === Main Processing Loop ===
def main():
    cap = cv2.VideoCapture(CAMERA_ID)
    if not cap.isOpened():
        logger.error(f"‚ùå Cannot access camera {CAMERA_ID}")
        return

    # Optimize camera settings
    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Reduce buffer for real-time processing
    cap.set(cv2.CAP_PROP_FPS, 30)
    
    logger.info("üì∑ Press 'q' to quit.")
    logger.info(f"üîç Using {'ResNet Crowd Density + YOLO + DeepSORT' if USE_RESNET else 'YOLO + DeepSORT'} on {device}")

    frame_count = 0
    csv_file = open(CSV_FILE, mode='a', newline='')
    csv_writer = csv.writer(csv_file)
    
    # Performance tracking
    processing_times = []
    
    try:
        while True:
            start_time = time.time()
            ret, frame = cap.read()
            if not ret:
                logger.warning("‚ö†Ô∏è Camera error, attempting to reconnect...")
                cap.release()
                time.sleep(2)
                cap = cv2.VideoCapture(CAMERA_ID)
                continue

            frame_count += 1
            if frame_count % FRAME_SKIP != 0:
                continue

            timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
            raw_filename = f"cam{CAMERA_ID}_{timestamp}.jpg"
            
            # === Crowd Density Analysis (NEW) ===
            crowd_count = 0.0
            crowd_level = "Unknown"
            crowd_confidence = 0.0
            
            if USE_RESNET:
                crowd_count, crowd_level, crowd_confidence = analyze_crowd_density(
                    frame, resnet_model, crowd_classifier, resnet_transform, device
                )

            # === Person Detection ===
            people_boxes = []
            crowded_people_count = 0
            
            # Run YOLO detection
            results = yolo_model(frame, verbose=False, conf=MIN_CONFIDENCE)
            
            # Collect crops for batch processing
            crops_for_resnet = []
            crop_indices = []
            
            for r in results:
                for box in r.boxes:
                    cls = int(box.cls[0])
                    conf = float(box.conf[0])
                    if yolo_model.names[cls] == "person":
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        
                        # Skip invalid boxes
                        if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 >= frame.shape[1] or y2 >= frame.shape[0]:
                            continue
                            
                        crop = frame[y1:y2, x1:x2]
                        if crop.size == 0:
                            continue

                        label = "YOLO"
                        final_conf = conf

                        # Check for potential crowding
                        bbox_area = (x2 - x1) * (y2 - y1)
                        is_potentially_crowded = conf < 0.6 or bbox_area < 1500

                        if USE_RESNET and is_potentially_crowded:
                            crops_for_resnet.append(crop)
                            crop_indices.append(len(people_boxes))
                            people_boxes.append((x1, y1, x2, y2, label, final_conf, True))  # True = needs ResNet
                        else:
                            people_boxes.append((x1, y1, x2, y2, label, final_conf, False))  # False = no ResNet needed

            # Batch process ResNet for individual people
            if USE_RESNET and crops_for_resnet:
                resnet_results = process_resnet_batch(crops_for_resnet, resnet_model, resnet_transform, device)
                
                # Update people_boxes with ResNet results
                for i, (prediction, confidence) in enumerate(resnet_results):
                    if prediction == 1 and confidence > 0.3:  # Person in crowded area
                        box_idx = crop_indices[i]
                        x1, y1, x2, y2, _, _, _ = people_boxes[box_idx]
                        people_boxes[box_idx] = (x1, y1, x2, y2, "Crowded Person", confidence, False)
                        crowded_people_count += 1

            # Clean up people_boxes (remove the ResNet flag)
            people_boxes = [(x1, y1, x2, y2, label, conf) for x1, y1, x2, y2, label, conf, flag in people_boxes if not flag or USE_RESNET]

            # === Tracking with DeepSORT ===
            formatted_detections = []
            for x1, y1, x2, y2, label, conf in people_boxes:
                formatted_detections.append(([x1, y1, x2, y2], conf, 0))  # 0 for person class

            tracks = tracker.update_tracks(formatted_detections, frame=frame)
            
            # Process tracks
            tracked_people = []
            active_track_ids = []
            for track in tracks:
                if not track.is_confirmed():
                    continue
                
                track_id = track.track_id
                ltrb = track.to_ltrb()
                x1, y1, x2, y2 = map(int, ltrb)
                active_track_ids.append(track_id)
                
                # Find best matching detection
                best_match = None
                best_iou = 0
                for det in people_boxes:
                    det_box = det[:4]
                    iou = calculate_iou(ltrb, det_box)
                    if iou > best_iou:
                        best_iou = iou
                        best_match = det
                
                if best_match and best_iou > 0.3:
                    label = f"{best_match[4]} (ID:{track_id})"
                    conf = best_match[5]
                else:
                    label = f"Tracked (ID:{track_id})"
                    conf = 0.7  # Default confidence for tracked objects
                
                tracked_people.append((x1, y1, x2, y2, label, conf))

            # Use tracked people count for density calculation
            people_count = len(tracked_people)
            density = people_count / CAMERA_FOV_M2
            device_status = "ON" if density > DENSITY_THRESHOLD else "OFF"

            # Control devices based on density
            try:
                if device_status == "ON":
                    turn_on_device()
                else:
                    turn_off_device()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è GPIO control error: {e}")

            # Log data
            processing_time = (time.time() - start_time) * 1000
            processing_times.append(processing_time)
            
            tracked_ids_str = ",".join(map(str, active_track_ids)) if active_track_ids else "None"
            csv_writer.writerow([
                timestamp.replace('_', ' '), 
                f"Camera {CAMERA_ID}",
                people_count, 
                f"{density:.4f}", 
                device_status, 
                f"{crowd_level} ({crowd_confidence:.2f})" if USE_RESNET else "N/A",
                tracked_ids_str,
                f"{processing_time:.2f}"
            ])

            # === Visualization ===
            for (x1, y1, x2, y2, label, conf) in tracked_people:
                # Color coding
                if "Crowded" in label:
                    color = (0, 0, 255)  # Red
                elif "Tracked" in label:
                    color = (0, 255, 0)  # Green
                else:
                    color = (255, 0, 0)  # Blue

                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(frame, f"{label} ({conf:.2f})", (x1, y1 - 5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

            # Enhanced status display
            avg_fps = 1000 / np.mean(processing_times[-30:]) if processing_times else 0
            cv2.putText(frame, f"People: {people_count} (Crowded: {crowded_people_count})",
                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            cv2.putText(frame, f"Density: {density:.4f} ppl/m¬≤ | Devices: {device_status}",
                        (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)
            cv2.putText(frame, f"Crowd Level: {crowd_level} ({crowd_confidence:.2f})" if USE_RESNET else "Crowd Level: N/A",
                        (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)
            cv2.putText(frame, f"Tracks: {len(active_track_ids)} | FPS: {avg_fps:.1f} | Device: {device}",
                        (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
            cv2.putText(frame, f"ResNet: {'ON' if USE_RESNET else 'OFF'} | Frame: {frame_count}",
                        (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            # Save and display
            cv2.imwrite(os.path.join(OUTPUT_DIR, raw_filename), frame)
            cv2.imshow("People Density Monitoring", frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

            # Periodic cleanup and logging
            if frame_count % 100 == 0:
                if device.type == 'mps':
                    torch.mps.empty_cache()
                avg_proc_time = np.mean(processing_times[-100:])
                logger.info(f"üìä Frame {frame_count}: Avg processing time: {avg_proc_time:.2f}ms, FPS: {1000/avg_proc_time:.1f}")

    except KeyboardInterrupt:
        logger.info("üõë Interrupted by user")
    finally:
        cap.release()
        cv2.destroyAllWindows()
        csv_file.close()
        cleanup_gpio()
        logger.info("üõë Cleanup completed")
        
        # Final statistics
        if processing_times:
            avg_time = np.mean(processing_times)
            logger.info(f"üìà Average processing time: {avg_time:.2f}ms")
            logger.info(f"üìà Average FPS: {1000/avg_time:.1f}")

if __name__ == "__main__":
    main()
    